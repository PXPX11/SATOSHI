{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from gluonts.dataset.split import split\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from utils.prerocess import read_stock_df\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 資料準備\n",
    "將資料讀取並抽取2010到2011的資料"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = read_stock_df('dataset/index/GSPC.csv', start_date='2010-01-01', end_date='2011-01-01',\n",
    "                   drops_columns=['Dividends', 'Stock Splits'])\n",
    "\n",
    "df\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "series_target = df['Close']\n",
    "\n",
    "dataset_std = np.nanstd(series_target)\n",
    "dataset_mean = np.nanmean(series_target)\n",
    "\n",
    "dataset_mean\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from gluonts.dataset.pandas import PandasDataset\n",
    "\n",
    "freq = \"1D\"\n",
    "dataset = PandasDataset(df, target=\"Close\", freq=freq, timestamp='Date')\n",
    "\n",
    "dataset\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from gluonts.dataset.util import to_pandas\n",
    "\n",
    "\n",
    "def highlight_entry(entry, color):\n",
    "    start = entry[\"start\"]\n",
    "    end = entry[\"start\"] + len(entry[\"target\"])\n",
    "    plt.axvspan(start, end, facecolor=color, alpha=0.2)\n",
    "\n",
    "\n",
    "def plot_dataset_splitting(original_dataset, training_dataset, test_pairs):\n",
    "    for original_entry, train_entry in zip(original_dataset, training_dataset):\n",
    "        to_pandas(original_entry).plot()\n",
    "        highlight_entry(train_entry, \"red\")\n",
    "        plt.legend([\"sub dataset\", \"training dataset\"], loc=\"upper left\")\n",
    "        plt.show()\n",
    "\n",
    "    for original_entry in original_dataset:\n",
    "        for test_input, test_label in test_pairs:\n",
    "            to_pandas(original_entry).plot()\n",
    "            highlight_entry(test_input, \"green\")\n",
    "            highlight_entry(test_label, \"blue\")\n",
    "            plt.legend([\"sub dataset\", \"test input\", \"test label\"], loc=\"upper left\")\n",
    "            plt.show()\n",
    "            break\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df['Date'].iloc[-72]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "length = len(df)\n",
    "split_size = length // 10 * 2\n",
    "split_size = len(df['Date'].iloc[-split_size:])\n",
    "split_date = df['Date'].iloc[-split_size]\n",
    "\n",
    "# 要弄圖的時候記得改成5\n",
    "prediction_length = 1\n",
    "windows = split_size // prediction_length - 1\n",
    "\n",
    "train_dataset, test_template = split(\n",
    "    dataset, date=pd.Period(split_date, freq=freq)\n",
    ")\n",
    "\n",
    "test_pairs = test_template.generate_instances(\n",
    "    prediction_length=prediction_length,\n",
    "    windows=windows,\n",
    ")\n",
    "\n",
    "plot_dataset_splitting(dataset, train_dataset, test_pairs)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# length = len(df)\n",
    "# split_size = length // 10 * 2\n",
    "#\n",
    "# prediction_length = 1\n",
    "#\n",
    "# train_dict = {\n",
    "#     'start': [df['Date'].iloc[0]],\n",
    "#     'target': [df['Close'][:-split_size]],\n",
    "# }\n",
    "#\n",
    "# test_dict = {\n",
    "#     'start': [df['Date'].iloc[0] for i in range(split_size)],\n",
    "#     'target': [df['Close'].iloc[:-split_size + i] for i in range(split_size)],\n",
    "# }\n",
    "#\n",
    "# train_dict\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# train_dataset = Dataset.from_dict(train_dict)\n",
    "# test_dataset = Dataset.from_dict(test_dict)\n",
    "\n",
    "# assert len(train_dataset[0]['target']) + prediction_length == len(validation_dataset[0]['target'])\n",
    "\n",
    "# figure, axes = plt.subplots()\n",
    "# axes.plot(train_dataset[0]['target'], color=\"blue\")\n",
    "# axes.plot(validation_dataset[0]['target'], color=\"red\", alpha=0.5)\n",
    "#\n",
    "# plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from gluonts.time_feature import get_lags_for_frequency\n",
    "\n",
    "lags_sequence = get_lags_for_frequency(freq)\n",
    "lags_sequence = lags_sequence[:10]\n",
    "lags_sequence\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from gluonts.time_feature import time_features_from_frequency_str\n",
    "\n",
    "time_features = time_features_from_frequency_str(freq)\n",
    "\n",
    "time_features\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import TimeSeriesTransformerConfig, TimeSeriesTransformerForPrediction\n",
    "\n",
    "context_length = 9\n",
    "\n",
    "config = TimeSeriesTransformerConfig(\n",
    "    prediction_length=prediction_length,\n",
    "    context_length=context_length,  # context length\n",
    "    lags_sequence=lags_sequence,\n",
    "    num_time_features=len(time_features) + 1,  # we'll add 2 time features (\"month of year\" and \"age\", see further)\n",
    "    num_static_categorical_features=1,  # we have a single static categorical feature, namely time series ID\n",
    "    cardinality=[len(train_dataset)],  # it has 366 possible values\n",
    "    embedding_dimension=[2],  # the model will learn an embedding of size 2 for each of the 366 possible values\n",
    "    encoder_layers=4,\n",
    "    decoder_layers=4,\n",
    ")\n",
    "\n",
    "model = TimeSeriesTransformerForPrediction(config)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define Transformations\n",
    "\n",
    "Next, we define the transformations for the data, in particular for the creation of the time features (based on the dataset or universal ones).\n",
    "\n",
    "Again, we'll use the GluonTS library for this. We define a `Chain` of transformations (which is a bit comparable to `torchvision.transforms.Compose` for images). It allows us to combine several transformations into a single pipeline."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from gluonts.time_feature import time_features_from_frequency_str\n",
    "from gluonts.dataset.field_names import FieldName\n",
    "from gluonts.transform import (\n",
    "    AddAgeFeature,\n",
    "    AddObservedValuesIndicator,\n",
    "    AddTimeFeatures,\n",
    "    AsNumpyArray,\n",
    "    Chain,\n",
    "    ExpectedNumInstanceSampler,\n",
    "    InstanceSplitter,\n",
    "    SelectFields,\n",
    "    SetField,\n",
    "    TestSplitSampler,\n",
    "    Transformation,\n",
    "    ValidationSplitSampler,\n",
    "    VstackFeatures,\n",
    "    RenameFields,\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import PretrainedConfig\n",
    "\n",
    "\n",
    "def create_transformation(freq: str, config: PretrainedConfig) -> Transformation:\n",
    "    # a bit like torchvision.transforms.Compose\n",
    "    return Chain(\n",
    "        [\n",
    "            # step 1: remove static/dynamic fields if not specified\n",
    "            # RemoveFields(field_names=['Dividends', 'Stock Splits']),\n",
    "\n",
    "            # step 2: use static features if available, if not add dummy values\n",
    "            SetField(output_field=FieldName.FEAT_STATIC_CAT, value=[0]),\n",
    "\n",
    "            SetField(output_field=FieldName.FEAT_STATIC_REAL, value=[0.0]),\n",
    "\n",
    "            # step 3: convert the data to NumPy (potentially not needed)\n",
    "            AsNumpyArray(\n",
    "                field=FieldName.FEAT_STATIC_CAT,\n",
    "                expected_ndim=1,\n",
    "                dtype=int,\n",
    "            ),\n",
    "            AsNumpyArray(\n",
    "                field=FieldName.FEAT_STATIC_REAL,\n",
    "                expected_ndim=1,\n",
    "            ),\n",
    "            # AsNumpyArray(\n",
    "            #     field='open',\n",
    "            #     # in the following line, we add 1 for the time dimension\n",
    "            #     expected_ndim=1,\n",
    "            # ),\n",
    "            AsNumpyArray(\n",
    "                field=FieldName.TARGET,\n",
    "                # in the following line, we add 1 for the time dimension\n",
    "                expected_ndim=1,\n",
    "            ),\n",
    "            # step 4: handle the NaN's by filling in the target with zero\n",
    "            # and return the mask (which is in the observed values)\n",
    "            # true for observed values, false for nan's\n",
    "            # the decoder uses this mask (no loss is incurred for unobserved values)\n",
    "            # see loss_weights inside the xxxForPrediction model\n",
    "            AddObservedValuesIndicator(\n",
    "                target_field=FieldName.TARGET,\n",
    "                output_field=FieldName.OBSERVED_VALUES,\n",
    "            ),\n",
    "\n",
    "            # step 5: add temporal features based on freq of the dataset\n",
    "            # month of year in this case\n",
    "            # these serve as positional encodings\n",
    "            # pure time feature, not target info inside\n",
    "            AddTimeFeatures(\n",
    "                start_field=FieldName.START,\n",
    "                target_field=FieldName.TARGET,\n",
    "                output_field=FieldName.FEAT_TIME,\n",
    "                time_features=time_features_from_frequency_str(freq),\n",
    "                pred_length=config.prediction_length,\n",
    "            ),\n",
    "\n",
    "            # step 6: add another temporal feature (just a single number)\n",
    "            # tells the model where in the life the value of the time series is\n",
    "            # sort of running counter\n",
    "            AddAgeFeature(\n",
    "                target_field=FieldName.TARGET,\n",
    "                output_field=FieldName.FEAT_AGE,\n",
    "                pred_length=config.prediction_length,\n",
    "                log_scale=True,\n",
    "            ),\n",
    "\n",
    "            # step 7: vertically stack all the temporal features\n",
    "            VstackFeatures(\n",
    "                output_field=FieldName.FEAT_TIME,\n",
    "                input_fields=[FieldName.FEAT_TIME, FieldName.FEAT_AGE],\n",
    "            ),\n",
    "\n",
    "            # step 8: rename to match HuggingFace names\n",
    "\n",
    "            RenameFields(\n",
    "                mapping={\n",
    "                    FieldName.FEAT_STATIC_CAT: \"static_categorical_features\",\n",
    "                    FieldName.FEAT_STATIC_REAL: \"static_real_features\",\n",
    "                    FieldName.FEAT_TIME: \"time_features\",\n",
    "                    FieldName.TARGET: \"values\",\n",
    "                    FieldName.OBSERVED_VALUES: \"observed_mask\",\n",
    "                }\n",
    "            ),\n",
    "        ]\n",
    "    )\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from gluonts.transform.sampler import InstanceSampler\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "def create_instance_splitter(config: PretrainedConfig, mode: str, train_sampler: Optional[InstanceSampler] = None,\n",
    "                             validation_sampler: Optional[InstanceSampler] = None, ) -> Transformation:\n",
    "    assert mode in [\"train\", \"validation\", \"test\"]\n",
    "\n",
    "    instance_sampler = {\n",
    "        \"train\": train_sampler or ExpectedNumInstanceSampler(\n",
    "            num_instances=1.0, min_future=config.prediction_length\n",
    "        ),\n",
    "        \"validation\": validation_sampler or ValidationSplitSampler(\n",
    "            min_future=config.prediction_length\n",
    "        ),\n",
    "        \"test\": TestSplitSampler(),\n",
    "    }[mode]\n",
    "\n",
    "    return InstanceSplitter(\n",
    "        target_field=\"values\",\n",
    "        is_pad_field=FieldName.IS_PAD,\n",
    "        start_field=FieldName.START,\n",
    "        forecast_start_field=FieldName.FORECAST_START,\n",
    "        instance_sampler=instance_sampler,\n",
    "        past_length=config.context_length + max(config.lags_sequence),\n",
    "        future_length=config.prediction_length,\n",
    "        time_series_fields=[\n",
    "            \"time_features\",\n",
    "            \"observed_mask\",\n",
    "        ],\n",
    "    )\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from gluonts.itertools import Cyclic, IterableSlice, PseudoShuffled\n",
    "from gluonts.torch.util import IterableDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from typing import Iterable\n",
    "\n",
    "\n",
    "def create_train_dataloader(\n",
    "        config: PretrainedConfig,\n",
    "        freq,\n",
    "        data,\n",
    "        batch_size: int,\n",
    "        num_batches_per_epoch: int,\n",
    "        shuffle_buffer_length: Optional[int] = None,\n",
    "        **kwargs,\n",
    ") -> Iterable:\n",
    "    PREDICTION_INPUT_NAMES = [\n",
    "        \"static_categorical_features\",\n",
    "        \"static_real_features\",\n",
    "        \"past_time_features\",\n",
    "        \"past_values\",\n",
    "        \"past_observed_mask\",\n",
    "        \"future_time_features\",\n",
    "    ]\n",
    "\n",
    "    TRAINING_INPUT_NAMES = PREDICTION_INPUT_NAMES + [\n",
    "        \"future_values\",\n",
    "        \"future_observed_mask\",\n",
    "    ]\n",
    "\n",
    "    transformation = create_transformation(freq, config)\n",
    "    transformed_data = transformation.apply(data, is_train=True)\n",
    "\n",
    "    # we initialize a Training instance\n",
    "    instance_splitter = create_instance_splitter(\n",
    "        config, \"train\"\n",
    "    ) + SelectFields(TRAINING_INPUT_NAMES)\n",
    "\n",
    "    # the instance splitter will sample a window of\n",
    "    # context length + lags + prediction length (from the 366 possible transformed time series)\n",
    "    # randomly from within the target time series and return an iterator.\n",
    "    training_instances = instance_splitter.apply(\n",
    "        Cyclic(transformed_data)\n",
    "        if shuffle_buffer_length is None\n",
    "        else PseudoShuffled(\n",
    "            Cyclic(transformed_data),\n",
    "            shuffle_buffer_length=shuffle_buffer_length,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # from the training instances iterator we now return a Dataloader which will\n",
    "    # continue to sample random windows for as long as it is called\n",
    "    # to return batch_size of the appropriate tensors ready for training!\n",
    "    return IterableSlice(\n",
    "        iter(\n",
    "            DataLoader(\n",
    "                IterableDataset(training_instances),\n",
    "                batch_size=batch_size,\n",
    "                **kwargs,\n",
    "            )\n",
    "        ),\n",
    "        num_batches_per_epoch,\n",
    "    )\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_dataloader = create_train_dataloader(\n",
    "    config=config,\n",
    "    freq=freq,\n",
    "    data=train_dataset,\n",
    "    batch_size=16,\n",
    "    num_batches_per_epoch=128,\n",
    ")\n",
    "\n",
    "batch = next(iter(train_dataloader))\n",
    "\n",
    "for k, v in batch.items():\n",
    "    print(\"{: >30} {: <30} {: >10}\".format(k, str(v.shape), v.type()))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_test_dataloader(\n",
    "        config: PretrainedConfig,\n",
    "        freq,\n",
    "        data,\n",
    "        batch_size: int,\n",
    "        **kwargs,\n",
    "):\n",
    "    PREDICTION_INPUT_NAMES = [\n",
    "        \"static_categorical_features\",\n",
    "        \"static_real_features\",\n",
    "        \"past_time_features\",\n",
    "        \"past_values\",\n",
    "        \"past_observed_mask\",\n",
    "        \"future_time_features\",\n",
    "    ]\n",
    "\n",
    "    new_data = [i for i, _ in data]\n",
    "\n",
    "    transformation = create_transformation(freq, config)\n",
    "    transformed_data = transformation.apply(new_data, is_train=False)\n",
    "\n",
    "    # we create a Test Instance splitter which will sample the very last\n",
    "    # context window seen during training only for the encoder.\n",
    "    instance_sampler = create_instance_splitter(\n",
    "        config, \"validation\"\n",
    "    ) + SelectFields(PREDICTION_INPUT_NAMES)\n",
    "\n",
    "    # we apply the transformations in test mode\n",
    "    testing_instances = instance_sampler.apply(transformed_data, is_train=False)\n",
    "\n",
    "    # This returns a Dataloader which will go over the dataset once.\n",
    "    return DataLoader(IterableDataset(testing_instances), batch_size=batch_size, **kwargs)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_dataloader = create_train_dataloader(\n",
    "    config=config,\n",
    "    freq=freq,\n",
    "    data=train_dataset,\n",
    "    batch_size=16,\n",
    "    num_batches_per_epoch=100,\n",
    ")\n",
    "\n",
    "# TODO test dataset has only one data now.\n",
    "test_dataloader = create_test_dataloader(\n",
    "    config=config,\n",
    "    freq=freq,\n",
    "    data=test_pairs,\n",
    "    batch_size=64,\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch = next(iter(test_dataloader))\n",
    "\n",
    "for k, v in batch.items():\n",
    "    print(\"{: >30} {: <30} {: >10}\".format(k, str(v.shape), v.type()))\n",
    "\n",
    "batch\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "\n",
    "for k, v in batch.items():\n",
    "    print(\"{: >30} {: <30} {: >10}\".format(k, str(v.shape), v.type()))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "outputs = model(\n",
    "    past_values=batch[\"past_values\"],\n",
    "    past_time_features=batch[\"past_time_features\"],\n",
    "    past_observed_mask=batch[\"past_observed_mask\"],\n",
    "    static_categorical_features=batch[\"static_categorical_features\"],\n",
    "    static_real_features=batch[\"static_real_features\"],\n",
    "    future_values=batch[\"future_values\"],\n",
    "    future_time_features=batch[\"future_time_features\"],\n",
    "    future_observed_mask=batch[\"future_observed_mask\"],\n",
    "    output_hidden_states=True\n",
    ")\n",
    "\n",
    "outputs\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Loss:\", outputs.loss.item())\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 8/1000 [00:22<46:18,  2.80s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[239], line 19\u001B[0m\n\u001B[1;32m     17\u001B[0m model\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m tqdm(\u001B[38;5;28mrange\u001B[39m(epoch_num)):\n\u001B[0;32m---> 19\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m batch \u001B[38;5;129;01min\u001B[39;00m train_dataloader:\n\u001B[1;32m     20\u001B[0m         optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m     21\u001B[0m         outputs \u001B[38;5;241m=\u001B[39m model(\n\u001B[1;32m     22\u001B[0m             static_categorical_features\u001B[38;5;241m=\u001B[39mbatch[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstatic_categorical_features\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mto(device),\n\u001B[1;32m     23\u001B[0m             static_real_features\u001B[38;5;241m=\u001B[39mbatch[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstatic_real_features\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mto(device),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     29\u001B[0m             future_observed_mask\u001B[38;5;241m=\u001B[39mbatch[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfuture_observed_mask\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mto(device),\n\u001B[1;32m     30\u001B[0m         )\n",
      "File \u001B[0;32m/opt/miniconda3/envs/NLP/lib/python3.10/site-packages/gluonts/itertools.py:181\u001B[0m, in \u001B[0;36mIterableSlice.__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    180\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__iter__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m--> 181\u001B[0m     \u001B[38;5;28;01myield from\u001B[39;00m itertools\u001B[38;5;241m.\u001B[39mislice(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39miterable, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlength)\n",
      "File \u001B[0;32m/opt/miniconda3/envs/NLP/lib/python3.10/site-packages/torch/utils/data/dataloader.py:628\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    625\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    626\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    627\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 628\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    629\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    630\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    631\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    632\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m/opt/miniconda3/envs/NLP/lib/python3.10/site-packages/torch/utils/data/dataloader.py:671\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    669\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    670\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 671\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    672\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    673\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[0;32m/opt/miniconda3/envs/NLP/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:34\u001B[0m, in \u001B[0;36m_IterableDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     32\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index:\n\u001B[1;32m     33\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 34\u001B[0m         data\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset_iter\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m     35\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n\u001B[1;32m     36\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mended \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[0;32m/opt/miniconda3/envs/NLP/lib/python3.10/site-packages/gluonts/torch/util.py:146\u001B[0m, in \u001B[0;36mIterableDataset.__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    145\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__iter__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m--> 146\u001B[0m     \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39miterable\n",
      "File \u001B[0;32m/opt/miniconda3/envs/NLP/lib/python3.10/site-packages/gluonts/transform/_base.py:103\u001B[0m, in \u001B[0;36mTransformedDataset.__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    102\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__iter__\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Iterator[DataEntry]:\n\u001B[0;32m--> 103\u001B[0m     \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransformation(\n\u001B[1;32m    104\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbase_dataset, is_train\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mis_train\n\u001B[1;32m    105\u001B[0m     )\n",
      "File \u001B[0;32m/opt/miniconda3/envs/NLP/lib/python3.10/site-packages/gluonts/transform/_base.py:124\u001B[0m, in \u001B[0;36mMapTransformation.__call__\u001B[0;34m(self, data_it, is_train)\u001B[0m\n\u001B[1;32m    121\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\n\u001B[1;32m    122\u001B[0m     \u001B[38;5;28mself\u001B[39m, data_it: Iterable[DataEntry], is_train: \u001B[38;5;28mbool\u001B[39m\n\u001B[1;32m    123\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Iterator:\n\u001B[0;32m--> 124\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m data_entry \u001B[38;5;129;01min\u001B[39;00m data_it:\n\u001B[1;32m    125\u001B[0m         \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    126\u001B[0m             \u001B[38;5;28;01myield\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmap_transform(data_entry\u001B[38;5;241m.\u001B[39mcopy(), is_train)\n",
      "File \u001B[0;32m/opt/miniconda3/envs/NLP/lib/python3.10/site-packages/gluonts/transform/_base.py:178\u001B[0m, in \u001B[0;36mFlatMapTransformation.__call__\u001B[0;34m(self, data_it, is_train)\u001B[0m\n\u001B[1;32m    174\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\n\u001B[1;32m    175\u001B[0m     \u001B[38;5;28mself\u001B[39m, data_it: Iterable[DataEntry], is_train: \u001B[38;5;28mbool\u001B[39m\n\u001B[1;32m    176\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Iterator:\n\u001B[1;32m    177\u001B[0m     num_idle_transforms \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m--> 178\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m data_entry \u001B[38;5;129;01min\u001B[39;00m data_it:\n\u001B[1;32m    179\u001B[0m         num_idle_transforms \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    180\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m result \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mflatmap_transform(data_entry\u001B[38;5;241m.\u001B[39mcopy(), is_train):\n",
      "File \u001B[0;32m/opt/miniconda3/envs/NLP/lib/python3.10/site-packages/gluonts/itertools.py:77\u001B[0m, in \u001B[0;36mCyclic.__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     75\u001B[0m at_least_one \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m     76\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m---> 77\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m el \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39miterable:\n\u001B[1;32m     78\u001B[0m         at_least_one \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m     79\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m el\n",
      "File \u001B[0;32m/opt/miniconda3/envs/NLP/lib/python3.10/site-packages/gluonts/transform/_base.py:103\u001B[0m, in \u001B[0;36mTransformedDataset.__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    102\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__iter__\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Iterator[DataEntry]:\n\u001B[0;32m--> 103\u001B[0m     \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransformation(\n\u001B[1;32m    104\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbase_dataset, is_train\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mis_train\n\u001B[1;32m    105\u001B[0m     )\n",
      "File \u001B[0;32m/opt/miniconda3/envs/NLP/lib/python3.10/site-packages/gluonts/transform/_base.py:124\u001B[0m, in \u001B[0;36mMapTransformation.__call__\u001B[0;34m(self, data_it, is_train)\u001B[0m\n\u001B[1;32m    121\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\n\u001B[1;32m    122\u001B[0m     \u001B[38;5;28mself\u001B[39m, data_it: Iterable[DataEntry], is_train: \u001B[38;5;28mbool\u001B[39m\n\u001B[1;32m    123\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Iterator:\n\u001B[0;32m--> 124\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m data_entry \u001B[38;5;129;01min\u001B[39;00m data_it:\n\u001B[1;32m    125\u001B[0m         \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    126\u001B[0m             \u001B[38;5;28;01myield\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmap_transform(data_entry\u001B[38;5;241m.\u001B[39mcopy(), is_train)\n",
      "File \u001B[0;32m/opt/miniconda3/envs/NLP/lib/python3.10/site-packages/gluonts/transform/_base.py:124\u001B[0m, in \u001B[0;36mMapTransformation.__call__\u001B[0;34m(self, data_it, is_train)\u001B[0m\n\u001B[1;32m    121\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\n\u001B[1;32m    122\u001B[0m     \u001B[38;5;28mself\u001B[39m, data_it: Iterable[DataEntry], is_train: \u001B[38;5;28mbool\u001B[39m\n\u001B[1;32m    123\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Iterator:\n\u001B[0;32m--> 124\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m data_entry \u001B[38;5;129;01min\u001B[39;00m data_it:\n\u001B[1;32m    125\u001B[0m         \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    126\u001B[0m             \u001B[38;5;28;01myield\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmap_transform(data_entry\u001B[38;5;241m.\u001B[39mcopy(), is_train)\n",
      "    \u001B[0;31m[... skipping similar frames: MapTransformation.__call__ at line 124 (7 times)]\u001B[0m\n",
      "File \u001B[0;32m/opt/miniconda3/envs/NLP/lib/python3.10/site-packages/gluonts/transform/_base.py:124\u001B[0m, in \u001B[0;36mMapTransformation.__call__\u001B[0;34m(self, data_it, is_train)\u001B[0m\n\u001B[1;32m    121\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\n\u001B[1;32m    122\u001B[0m     \u001B[38;5;28mself\u001B[39m, data_it: Iterable[DataEntry], is_train: \u001B[38;5;28mbool\u001B[39m\n\u001B[1;32m    123\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Iterator:\n\u001B[0;32m--> 124\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m data_entry \u001B[38;5;129;01min\u001B[39;00m data_it:\n\u001B[1;32m    125\u001B[0m         \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    126\u001B[0m             \u001B[38;5;28;01myield\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmap_transform(data_entry\u001B[38;5;241m.\u001B[39mcopy(), is_train)\n",
      "File \u001B[0;32m/opt/miniconda3/envs/NLP/lib/python3.10/site-packages/gluonts/dataset/split.py:258\u001B[0m, in \u001B[0;36mAbstractBaseSplitter.generate_training_entries\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    255\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mgenerate_training_entries\u001B[39m(\n\u001B[1;32m    256\u001B[0m     \u001B[38;5;28mself\u001B[39m, dataset: Dataset\n\u001B[1;32m    257\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Generator[DataEntry, \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m]:\n\u001B[0;32m--> 258\u001B[0m     \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mmap\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining_entry, dataset)\n",
      "File \u001B[0;32m/opt/miniconda3/envs/NLP/lib/python3.10/site-packages/gluonts/dataset/pandas.py:200\u001B[0m, in \u001B[0;36mPandasDataset.__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    199\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__iter__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m--> 200\u001B[0m     \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_data_entries\n\u001B[1;32m    201\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39munchecked \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[0;32m/opt/miniconda3/envs/NLP/lib/python3.10/site-packages/gluonts/dataset/pandas.py:154\u001B[0m, in \u001B[0;36mPandasDataset._pair_to_dataentry\u001B[0;34m(self, item_id, df)\u001B[0m\n\u001B[1;32m    151\u001B[0m     df \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mto_frame(name\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget)\n\u001B[1;32m    153\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtimestamp:\n\u001B[0;32m--> 154\u001B[0m     df\u001B[38;5;241m.\u001B[39mindex \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mPeriodIndex\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtimestamp\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfreq\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfreq\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    156\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(df\u001B[38;5;241m.\u001B[39mindex, pd\u001B[38;5;241m.\u001B[39mPeriodIndex):\n\u001B[1;32m    157\u001B[0m     df \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mto_period(freq\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfreq)\n",
      "File \u001B[0;32m/opt/miniconda3/envs/NLP/lib/python3.10/site-packages/pandas/core/indexes/period.py:273\u001B[0m, in \u001B[0;36mPeriodIndex.__new__\u001B[0;34m(cls, data, ordinal, freq, dtype, copy, name, **fields)\u001B[0m\n\u001B[1;32m    270\u001B[0m         data \u001B[38;5;241m=\u001B[39m PeriodArray(ordinal, freq\u001B[38;5;241m=\u001B[39mfreq)\n\u001B[1;32m    271\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    272\u001B[0m         \u001B[38;5;66;03m# don't pass copy here, since we copy later.\u001B[39;00m\n\u001B[0;32m--> 273\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[43mperiod_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfreq\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfreq\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    275\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m copy:\n\u001B[1;32m    276\u001B[0m     data \u001B[38;5;241m=\u001B[39m data\u001B[38;5;241m.\u001B[39mcopy()\n",
      "File \u001B[0;32m/opt/miniconda3/envs/NLP/lib/python3.10/site-packages/pandas/core/arrays/period.py:977\u001B[0m, in \u001B[0;36mperiod_array\u001B[0;34m(data, freq, copy)\u001B[0m\n\u001B[1;32m    973\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m PeriodArray(ordinals, dtype\u001B[38;5;241m=\u001B[39mdtype)\n\u001B[1;32m    975\u001B[0m data \u001B[38;5;241m=\u001B[39m ensure_object(arrdata)\n\u001B[0;32m--> 977\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mPeriodArray\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_from_sequence\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/miniconda3/envs/NLP/lib/python3.10/site-packages/pandas/core/arrays/period.py:274\u001B[0m, in \u001B[0;36mPeriodArray._from_sequence\u001B[0;34m(cls, scalars, dtype, copy)\u001B[0m\n\u001B[1;32m    271\u001B[0m periods \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39masarray(scalars, dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mobject\u001B[39m)\n\u001B[1;32m    273\u001B[0m freq \u001B[38;5;241m=\u001B[39m freq \u001B[38;5;129;01mor\u001B[39;00m libperiod\u001B[38;5;241m.\u001B[39mextract_freq(periods)\n\u001B[0;32m--> 274\u001B[0m ordinals \u001B[38;5;241m=\u001B[39m \u001B[43mlibperiod\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mextract_ordinals\u001B[49m\u001B[43m(\u001B[49m\u001B[43mperiods\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfreq\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    275\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mcls\u001B[39m(ordinals, freq\u001B[38;5;241m=\u001B[39mfreq)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from accelerate import Accelerator\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "\n",
    "accelerator = Accelerator()\n",
    "device = accelerator.device\n",
    "\n",
    "model.to(device)\n",
    "optimizer = Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "model, optimizer, train_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader,\n",
    ")\n",
    "\n",
    "epoch_num = 1000\n",
    "\n",
    "model.train()\n",
    "for epoch in tqdm(range(epoch_num)):\n",
    "    for batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(\n",
    "            static_categorical_features=batch[\"static_categorical_features\"].to(device),\n",
    "            static_real_features=batch[\"static_real_features\"].to(device),\n",
    "            past_time_features=batch[\"past_time_features\"].to(device),\n",
    "            past_values=batch[\"past_values\"].to(device),\n",
    "            future_time_features=batch[\"future_time_features\"].to(device),\n",
    "            future_values=batch[\"future_values\"].to(device),\n",
    "            past_observed_mask=batch[\"past_observed_mask\"].to(device),\n",
    "            future_observed_mask=batch[\"future_observed_mask\"].to(device),\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backpropagation\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "\n",
    "        # print(loss.item())\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.save(model.state_dict(), 'weight/test.pt')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "forecasts = []\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    outputs = model.generate(\n",
    "        static_categorical_features=batch[\"static_categorical_features\"].to(device),\n",
    "        static_real_features=batch[\"static_real_features\"].to(device),\n",
    "        past_time_features=batch[\"past_time_features\"].to(device),\n",
    "        past_values=batch[\"past_values\"].to(device),\n",
    "        future_time_features=batch[\"future_time_features\"].to(device),\n",
    "        past_observed_mask=batch[\"past_observed_mask\"].to(device),\n",
    "    )\n",
    "    forecasts.append(outputs.sequences.cpu().numpy())\n",
    "\n",
    "forecasts = np.vstack(forecasts)\n",
    "forecasts.shape\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "forecast_median = np.median(forecasts, 1)\n",
    "forecast_median_standardized = (forecast_median - dataset_mean) / dataset_std\n",
    "forecast_median_standardized = forecast_median_standardized[:, 0]\n",
    "\n",
    "# np.median(forecasts[0][0])\n",
    "# forecast_median.shape\n",
    "forecast_median_standardized.shape\n",
    "\n",
    "# forecasts.shape\n",
    "# forecasts[:, 0, :, 0]\n",
    "# np.median(forecasts[:, 0])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "\n",
    "mase_metric = load(\"evaluate-metric/mase\")\n",
    "smape_metric = load(\"evaluate-metric/smape\")\n",
    "\n",
    "mae_metric = load(\"evaluate-metric/mae\")\n",
    "mse_metric = load(\"evaluate-metric/mse\")\n",
    "mape_metric = load(\"evaluate-metric/mape\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ground_truth = [test_label['target'][0] for test_input, test_label in test_pairs]\n",
    "ground_truth = (ground_truth - dataset_mean) / dataset_std\n",
    "ground_truth = np.array(ground_truth)\n",
    "\n",
    "is_finite = np.isfinite(ground_truth)\n",
    "\n",
    "predictions = forecast_median_standardized[is_finite]\n",
    "references = ground_truth[is_finite]\n",
    "\n",
    "mae = mae_metric.compute(\n",
    "    predictions=predictions,\n",
    "    references=references,\n",
    ")\n",
    "mae_metrics = mae[\"mae\"]\n",
    "\n",
    "mse = mse_metric.compute(\n",
    "    predictions=predictions,\n",
    "    references=references,\n",
    "    squared=False,\n",
    ")\n",
    "mse_metrics = mse[\"mse\"]\n",
    "\n",
    "mape = mape_metric.compute(\n",
    "    predictions=predictions,\n",
    "    references=references,\n",
    ")\n",
    "mape_metrics = mape[\"mape\"]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f\"MAE: {mae_metrics}\")\n",
    "print(f\"RMSE: {mse_metrics}\")\n",
    "print(f\"MAPE: {mape_metrics}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.dates as mdates\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "display_size = split_size * 2\n",
    "actual_value = df['Close'][-display_size:]\n",
    "# actual_value = test_dataset[FieldName.TARGET][-1][-display_size:]\n",
    "\n",
    "df_actual_value = pd.DataFrame(actual_value)\n",
    "df_actual_value = df_actual_value.fillna(method='ffill')\n",
    "\n",
    "index = pd.period_range(\n",
    "    # start=test_pairs[FieldName.START][0],\n",
    "    start=split_date,\n",
    "    periods=len(actual_value),\n",
    "    freq=freq,\n",
    ").to_timestamp()\n",
    "\n",
    "# Major ticks every half year, minor ticks every month,\n",
    "ax.xaxis.set_major_locator(mdates.MonthLocator(bymonth=(1, 13)))\n",
    "ax.xaxis.set_minor_locator(mdates.MonthLocator())\n",
    "\n",
    "ax.plot(\n",
    "    index,\n",
    "    df_actual_value,\n",
    "    label=\"actual\",\n",
    ")\n",
    "\n",
    "plt.plot(\n",
    "    index[-split_size + 1:],\n",
    "    np.median(forecasts, axis=1),\n",
    "    label=\"median\",\n",
    ")\n",
    "\n",
    "forecasts_mean = np.mean(forecasts, axis=1)[:, 0]\n",
    "forecasts_std = np.std(forecasts, axis=1)[:, 0]\n",
    "\n",
    "plt.fill_between(\n",
    "    index[-split_size + 1:],\n",
    "    forecasts_mean - forecasts_std,\n",
    "    forecasts_mean + forecasts_std,\n",
    "    alpha=0.3,\n",
    "    interpolate=True,\n",
    "    label=\"+/- 1-std\",\n",
    ")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Major ticks every half year, minor ticks every month,\n",
    "# ax.xaxis.set_major_locator(mdates.MonthLocator(bymonth=(1, 7)))\n",
    "# ax.xaxis.set_minor_locator(mdates.MonthLocator())\n",
    "#\n",
    "# ax.plot(\n",
    "#     index[-2 * prediction_length:],\n",
    "#     test_dataset[ts_index][\"target\"][-2 * prediction_length:],\n",
    "#     label=\"actual\",\n",
    "# )\n",
    "#\n",
    "# plt.plot(\n",
    "#     index[-prediction_length:],\n",
    "#     np.median(forecasts[ts_index], axis=0),\n",
    "#     label=\"median\",\n",
    "# )\n",
    "#\n",
    "# plt.fill_between(\n",
    "#     index[-prediction_length:],\n",
    "#     forecasts[ts_index].mean(0) - forecasts[ts_index].std(axis=0),\n",
    "#     forecasts[ts_index].mean(0) + forecasts[ts_index].std(axis=0),\n",
    "#     alpha=0.3,\n",
    "#     interpolate=True,\n",
    "#     label=\"+/- 1-std\",\n",
    "# )\n",
    "# plt.legend()\n",
    "# plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
